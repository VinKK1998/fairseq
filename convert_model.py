from transformers import HubertModel, HubertConfig
import torch
from prettytable import PrettyTable
import fairseq
from fairseq.models.hubert import HubertModel as fair_hubert
from fairseq.models.hubert import HubertConfig as fair_config
from fairseq.models.hubert import HubertPretrainingConfig


def count_parameters(model):
    table = PrettyTable(["Modules", "Parameters"])
    total_params = 0
    for name, parameter in model.named_parameters():
        print(name)
        if not parameter.requires_grad:
            continue
        param = parameter.numel()
        table.add_row([name, param])
        total_params += param
    print(table)
    print(f"Total Trainable Params: {total_params}")
    return total_params


if __name__ == '__main__':
    fairseq_model_path = 'outputs/2021-11-08/14-39-01/checkpoints/checkpoint_last.pt'
    fairseq_model = torch.load(fairseq_model_path)
    model_export = fairseq_model['model']
    transformers_model_config = HubertConfig()
    model = HubertModel(config=transformers_model_config)
    # with open('./fairseq_hubert.struc') as f1:
    #     fair_keys = [i.strip() for i in f1.readlines()]
    # with open('./trans_hubert.struc') as f1:
    #     trans_keys = [i.strip() for i in f1.readlines()]
    fair_keys = ['label_embs_concat', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias',
                 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.6.fc1.weight', 'encoder.layers.6.fc1.bias', 'encoder.layers.6.fc2.weight', 'encoder.layers.6.fc2.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.7.fc1.weight', 'encoder.layers.7.fc1.bias', 'encoder.layers.7.fc2.weight', 'encoder.layers.7.fc2.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.8.self_attn_layer_norm.weight', 'encoder.layers.8.self_attn_layer_norm.bias', 'encoder.layers.8.fc1.weight', 'encoder.layers.8.fc1.bias', 'encoder.layers.8.fc2.weight', 'encoder.layers.8.fc2.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.9.self_attn_layer_norm.weight', 'encoder.layers.9.self_attn_layer_norm.bias', 'encoder.layers.9.fc1.weight', 'encoder.layers.9.fc1.bias', 'encoder.layers.9.fc2.weight', 'encoder.layers.9.fc2.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.10.self_attn_layer_norm.weight', 'encoder.layers.10.self_attn_layer_norm.bias', 'encoder.layers.10.fc1.weight', 'encoder.layers.10.fc1.bias', 'encoder.layers.10.fc2.weight', 'encoder.layers.10.fc2.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.11.self_attn_layer_norm.weight', 'encoder.layers.11.self_attn_layer_norm.bias', 'encoder.layers.11.fc1.weight', 'encoder.layers.11.fc1.bias', 'encoder.layers.11.fc2.weight', 'encoder.layers.11.fc2.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layer_norm.bias', 'layer_norm.weight', 'layer_norm.bias', 'final_proj.weight', 'final_proj.bias']
    
    cnn_keys = ['feature_extractor.conv_layers.0.conv.weight', 'feature_extractor.conv_layers.1.conv.weight', 'feature_extractor.conv_layers.2.conv.weight', 'feature_extractor.conv_layers.3.conv.weight', 'feature_extractor.conv_layers.4.conv.weight', 'feature_extractor.conv_layers.5.conv.weight', 'feature_extractor.conv_layers.6.conv.weight']
    
    proj_keys = ['feature_projection.layer_norm.weight', 'feature_projection.layer_norm.bias', 'feature_projection.projection.weight', 'feature_projection.projection.bias']

    trans_keys = ['encoder.layers.0.attention.k_proj.weight', 'encoder.layers.0.attention.k_proj.bias', 'encoder.layers.0.attention.v_proj.weight', 'encoder.layers.0.attention.v_proj.bias', 'encoder.layers.0.attention.q_proj.weight', 'encoder.layers.0.attention.q_proj.bias', 'encoder.layers.0.attention.out_proj.weight', 'encoder.layers.0.attention.out_proj.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.feed_forward.intermediate_dense.weight', 'encoder.layers.0.feed_forward.intermediate_dense.bias', 'encoder.layers.0.feed_forward.output_dense.weight', 'encoder.layers.0.feed_forward.output_dense.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.attention.k_proj.weight', 'encoder.layers.1.attention.k_proj.bias', 'encoder.layers.1.attention.v_proj.weight', 'encoder.layers.1.attention.v_proj.bias', 'encoder.layers.1.attention.q_proj.weight', 'encoder.layers.1.attention.q_proj.bias', 'encoder.layers.1.attention.out_proj.weight', 'encoder.layers.1.attention.out_proj.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.feed_forward.intermediate_dense.weight', 'encoder.layers.1.feed_forward.intermediate_dense.bias', 'encoder.layers.1.feed_forward.output_dense.weight', 'encoder.layers.1.feed_forward.output_dense.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.attention.k_proj.weight', 'encoder.layers.2.attention.k_proj.bias', 'encoder.layers.2.attention.v_proj.weight', 'encoder.layers.2.attention.v_proj.bias', 'encoder.layers.2.attention.q_proj.weight', 'encoder.layers.2.attention.q_proj.bias', 'encoder.layers.2.attention.out_proj.weight', 'encoder.layers.2.attention.out_proj.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.feed_forward.intermediate_dense.weight', 'encoder.layers.2.feed_forward.intermediate_dense.bias', 'encoder.layers.2.feed_forward.output_dense.weight', 'encoder.layers.2.feed_forward.output_dense.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.attention.k_proj.weight', 'encoder.layers.3.attention.k_proj.bias', 'encoder.layers.3.attention.v_proj.weight', 'encoder.layers.3.attention.v_proj.bias', 'encoder.layers.3.attention.q_proj.weight', 'encoder.layers.3.attention.q_proj.bias', 'encoder.layers.3.attention.out_proj.weight', 'encoder.layers.3.attention.out_proj.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.feed_forward.intermediate_dense.weight', 'encoder.layers.3.feed_forward.intermediate_dense.bias', 'encoder.layers.3.feed_forward.output_dense.weight', 'encoder.layers.3.feed_forward.output_dense.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.attention.k_proj.weight', 'encoder.layers.4.attention.k_proj.bias', 'encoder.layers.4.attention.v_proj.weight', 'encoder.layers.4.attention.v_proj.bias', 'encoder.layers.4.attention.q_proj.weight', 'encoder.layers.4.attention.q_proj.bias', 'encoder.layers.4.attention.out_proj.weight', 'encoder.layers.4.attention.out_proj.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.feed_forward.intermediate_dense.weight', 'encoder.layers.4.feed_forward.intermediate_dense.bias', 'encoder.layers.4.feed_forward.output_dense.weight', 'encoder.layers.4.feed_forward.output_dense.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.out_proj.bias',
                  'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.final_layer_norm.bias']
    
    model_new = {}
    model_new['masked_spec_embed'] = model_export['mask_emb']
    model_new['feature_extractor.conv_layers.0.layer_norm.weight'] = model_export['feature_extractor.conv_layers.0.2.weight']
    model_new['feature_extractor.conv_layers.0.layer_norm.bias'] = model_export['feature_extractor.conv_layers.0.2.bias']
    
    for cnn_key in cnn_keys:
        model_new[cnn_key] = model_export[cnn_key.replace('conv.weight', '0.weight')]
    
    model_new['feature_projection.projection.weight'] = model_export['post_extract_proj.weight']
    model_new['feature_projection.projection.bias'] = model_export['post_extract_proj.bias']
    
    model_new['encoder.pos_conv_embed.conv.bias'] = model_export['encoder.pos_conv.0.bias']
    model_new['encoder.pos_conv_embed.conv.weight_g'] = model_export['encoder.pos_conv.0.weight_g']
    model_new['encoder.pos_conv_embed.conv.weight_v'] = model_export['encoder.pos_conv.0.weight_v']
    
    model_new['encoder.layer_norm.weight'] = model_export['layer_norm.weight']
    model_new['encoder.layer_norm.bias'] = model_export['layer_norm.bias']

    for i in range(12):
        for key in ['k_proj', 'v_proj', 'q_proj', 'out_proj']:
            for value in ['weight', 'bias']:
                model_new['encoder.layers.{}.attention.{}.{}'.format(i, key, value)] = model_export['encoder.layers.{}.self_attn.{}.{}'.format(i, key, value)]
        model_new['encoder.layers.{}.layer_norm.weight'.format(i)] = model_export['encoder.layers.{}.self_attn_layer_norm.weight'.format(i)]
        model_new['encoder.layers.{}.layer_norm.bias'.format(i)] = model_export['encoder.layers.{}.self_attn_layer_norm.bias'.format(i)]
        model_new['encoder.layers.{}.feed_forward.intermediate_dense.weight'.format(i)] = model_export['encoder.layers.{}.fc1.weight'.format(i)]
        model_new['encoder.layers.{}.feed_forward.intermediate_dense.bias'.format(i)] = model_export['encoder.layers.{}.fc1.bias'.format(i)]
        model_new['encoder.layers.{}.feed_forward.output_dense.weight'.format(i)] = model_export['encoder.layers.{}.fc2.weight'.format(i)]
        model_new['encoder.layers.{}.feed_forward.output_dense.bias'.format(i)] = model_export['encoder.layers.{}.fc2.bias'.format(i)]
        model_new['encoder.layers.{}.final_layer_norm.weight'.format(i)] = model_export['encoder.layers.{}.final_layer_norm.weight'.format(i)]
        model_new['encoder.layers.{}.final_layer_norm.bias'.format(i)] = model_export['encoder.layers.{}.final_layer_norm.bias'.format(i)]
    
    model.load_state_dict(model_new)





    
